{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "\n",
    "# inserimos um texto aleatório\n",
    "texto = \"\"\"France, in Western Europe, encompasses medieval cities, alpine villages and Mediterranean beaches. Paris, its capital, is famed for its fashion houses, classical art museums including the Louvre and monuments like the Eiffel Tower. The country is also renowned for its wines and sophisticated cuisine. Lascaux’s ancient cave drawings, Lyon’s Roman theater and the vast Palace of Versailles attest to its rich history.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Vinicius\\AppData\\Roaming\\nltk_data...\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Vinicius/nltk_data'\n    - 'C:\\\\Users\\\\Vinicius\\\\Anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\Vinicius\\\\Anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\Vinicius\\\\Anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Vinicius\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-598a4d71fea0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Divide o texto em frases\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msent_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtexto\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\tokenize\\__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m    102\u001b[0m     \u001b[1;33m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m     \"\"\"\n\u001b[1;32m--> 104\u001b[1;33m     \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'tokenizers/punkt/{0}.pickle'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    105\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[0;32m    866\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    867\u001b[0m     \u001b[1;31m# Load the resource.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 868\u001b[1;33m     \u001b[0mopened_resource\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    869\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    870\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'raw'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36m_open\u001b[1;34m(resource_url)\u001b[0m\n\u001b[0;32m    991\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    992\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'nltk'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 993\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    994\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'file'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    995\u001b[0m         \u001b[1;31m# urllib might not use mode='rb', so handle this one ourselves:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    697\u001b[0m     \u001b[0msep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'*'\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m70\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    698\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 699\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    700\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    701\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Vinicius/nltk_data'\n    - 'C:\\\\Users\\\\Vinicius\\\\Anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\Vinicius\\\\Anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\Vinicius\\\\Anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Vinicius\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "# Divide o texto em frases\n",
    "dataset = nltk.sent_tokenize(texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['france in western europe encompasses medieval cities alpine villages and mediterranean beaches ', 'paris its capital is famed for its fashion houses classical art museums including the louvre and monuments like the eiffel tower ', 'the country is also renowned for its wines and sophisticated cuisine ', 'lascaux s ancient cave drawings lyon s roman theater and the vast palace of versailles attest to its rich history ']\n"
     ]
    }
   ],
   "source": [
    "#realizando o pré-processamento\n",
    "for i in range (len(dataset)):\n",
    "    dataset[i] = dataset[i].lower() #converte todas as palavras para letras minusculas\n",
    "    dataset[i] = re.sub(r'\\W', ' ', dataset[i]) # troca tudo que não for uma palavra para um espaço\n",
    "    dataset[i] = re.sub(r'\\s+', ' ', dataset[i]) # troca tudo que for quebras de linha para um espaço simples\n",
    "    \n",
    "print (dataset)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'france': 1, 'in': 1, 'western': 1, 'europe': 1, 'encompasses': 1, 'medieval': 1, 'cities': 1, 'alpine': 1, 'villages': 1, 'and': 4, 'mediterranean': 1, 'beaches': 1, 'paris': 1, 'its': 4, 'capital': 1, 'is': 2, 'famed': 1, 'for': 2, 'fashion': 1, 'houses': 1, 'classical': 1, 'art': 1, 'museums': 1, 'including': 1, 'the': 4, 'louvre': 1, 'monuments': 1, 'like': 1, 'eiffel': 1, 'tower': 1, 'country': 1, 'also': 1, 'renowned': 1, 'wines': 1, 'sophisticated': 1, 'cuisine': 1, 'lascaux': 1, 's': 2, 'ancient': 1, 'cave': 1, 'drawings': 1, 'lyon': 1, 'roman': 1, 'theater': 1, 'vast': 1, 'palace': 1, 'of': 1, 'versailles': 1, 'attest': 1, 'to': 1, 'rich': 1, 'history': 1}\n"
     ]
    }
   ],
   "source": [
    "# criando um histograma\n",
    "\n",
    "word2count = {}\n",
    "for data in dataset:\n",
    "    words = nltk.word_tokenize(data)\n",
    "    for word in words:\n",
    "        if word not in word2count.keys():\n",
    "            word2count[word] = 1\n",
    "        else:\n",
    "            word2count[word] += 1\n",
    "            \n",
    "print (word2count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'its', 'the', 'is', 'for', 's', 'france', 'in', 'western', 'europe', 'encompasses', 'medieval', 'cities', 'alpine', 'villages', 'mediterranean', 'beaches', 'paris', 'capital', 'famed', 'fashion', 'houses', 'classical', 'art', 'museums', 'including', 'louvre', 'monuments', 'like', 'eiffel', 'tower', 'country', 'also', 'renowned', 'wines', 'sophisticated', 'cuisine', 'lascaux', 'ancient', 'cave', 'drawings', 'lyon', 'roman', 'theater', 'vast', 'palace', 'of', 'versailles', 'attest', 'to']\n"
     ]
    }
   ],
   "source": [
    "# biblioteca para ordenação\n",
    "import heapq\n",
    "\n",
    "# ordena a lista para saber qual é a palavra que mais se repete\n",
    "freq_words = heapq.nlargest(50,word2count, key=word2count.get)\n",
    "print (freq_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'and': 0.6931471805599453, 'its': 0.8472978603872034, 'the': 0.8472978603872034, 'is': 1.0986122886681098, 'for': 1.0986122886681098, 's': 1.6094379124341003, 'france': 1.6094379124341003, 'in': 1.6094379124341003, 'western': 1.6094379124341003, 'europe': 1.6094379124341003, 'encompasses': 1.6094379124341003, 'medieval': 1.6094379124341003, 'cities': 1.6094379124341003, 'alpine': 1.6094379124341003, 'villages': 1.6094379124341003, 'mediterranean': 1.6094379124341003, 'beaches': 1.6094379124341003, 'paris': 1.6094379124341003, 'capital': 1.6094379124341003, 'famed': 1.6094379124341003, 'fashion': 1.6094379124341003, 'houses': 1.6094379124341003, 'classical': 1.6094379124341003, 'art': 1.6094379124341003, 'museums': 1.6094379124341003, 'including': 1.6094379124341003, 'louvre': 1.6094379124341003, 'monuments': 1.6094379124341003, 'like': 1.6094379124341003, 'eiffel': 1.6094379124341003, 'tower': 1.6094379124341003, 'country': 1.6094379124341003, 'also': 1.6094379124341003, 'renowned': 1.6094379124341003, 'wines': 1.6094379124341003, 'sophisticated': 1.6094379124341003, 'cuisine': 1.6094379124341003, 'lascaux': 1.6094379124341003, 'ancient': 1.6094379124341003, 'cave': 1.6094379124341003, 'drawings': 1.6094379124341003, 'lyon': 1.6094379124341003, 'roman': 1.6094379124341003, 'theater': 1.6094379124341003, 'vast': 1.6094379124341003, 'palace': 1.6094379124341003, 'of': 1.6094379124341003, 'versailles': 1.6094379124341003, 'attest': 1.6094379124341003, 'to': 1.6094379124341003}\n"
     ]
    }
   ],
   "source": [
    "# criando o IDF\n",
    "import numpy as np \n",
    "\n",
    "word_idfs = {}\n",
    "for word in freq_words:\n",
    "    doc_count = 0 \n",
    "    for data in dataset:\n",
    "        if word in nltk.word_tokenize(data):\n",
    "            doc_count += 1\n",
    "    word_idfs[word] = np.log((len(dataset)/doc_count)+1)\n",
    "    \n",
    "print (word_idfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'and': [0.08333333333333333, 0.047619047619047616, 0.09090909090909091, 0.05], 'its': [0.0, 0.09523809523809523, 0.09090909090909091, 0.05], 'the': [0.0, 0.09523809523809523, 0.09090909090909091, 0.05], 'is': [0.0, 0.047619047619047616, 0.09090909090909091, 0.0], 'for': [0.0, 0.047619047619047616, 0.09090909090909091, 0.0], 's': [0.0, 0.0, 0.0, 0.1], 'france': [0.08333333333333333, 0.0, 0.0, 0.0], 'in': [0.08333333333333333, 0.0, 0.0, 0.0], 'western': [0.08333333333333333, 0.0, 0.0, 0.0], 'europe': [0.08333333333333333, 0.0, 0.0, 0.0], 'encompasses': [0.08333333333333333, 0.0, 0.0, 0.0], 'medieval': [0.08333333333333333, 0.0, 0.0, 0.0], 'cities': [0.08333333333333333, 0.0, 0.0, 0.0], 'alpine': [0.08333333333333333, 0.0, 0.0, 0.0], 'villages': [0.08333333333333333, 0.0, 0.0, 0.0], 'mediterranean': [0.08333333333333333, 0.0, 0.0, 0.0], 'beaches': [0.08333333333333333, 0.0, 0.0, 0.0], 'paris': [0.0, 0.047619047619047616, 0.0, 0.0], 'capital': [0.0, 0.047619047619047616, 0.0, 0.0], 'famed': [0.0, 0.047619047619047616, 0.0, 0.0], 'fashion': [0.0, 0.047619047619047616, 0.0, 0.0], 'houses': [0.0, 0.047619047619047616, 0.0, 0.0], 'classical': [0.0, 0.047619047619047616, 0.0, 0.0], 'art': [0.0, 0.047619047619047616, 0.0, 0.0], 'museums': [0.0, 0.047619047619047616, 0.0, 0.0], 'including': [0.0, 0.047619047619047616, 0.0, 0.0], 'louvre': [0.0, 0.047619047619047616, 0.0, 0.0], 'monuments': [0.0, 0.047619047619047616, 0.0, 0.0], 'like': [0.0, 0.047619047619047616, 0.0, 0.0], 'eiffel': [0.0, 0.047619047619047616, 0.0, 0.0], 'tower': [0.0, 0.047619047619047616, 0.0, 0.0], 'country': [0.0, 0.0, 0.09090909090909091, 0.0], 'also': [0.0, 0.0, 0.09090909090909091, 0.0], 'renowned': [0.0, 0.0, 0.09090909090909091, 0.0], 'wines': [0.0, 0.0, 0.09090909090909091, 0.0], 'sophisticated': [0.0, 0.0, 0.09090909090909091, 0.0], 'cuisine': [0.0, 0.0, 0.09090909090909091, 0.0], 'lascaux': [0.0, 0.0, 0.0, 0.05], 'ancient': [0.0, 0.0, 0.0, 0.05], 'cave': [0.0, 0.0, 0.0, 0.05], 'drawings': [0.0, 0.0, 0.0, 0.05], 'lyon': [0.0, 0.0, 0.0, 0.05], 'roman': [0.0, 0.0, 0.0, 0.05], 'theater': [0.0, 0.0, 0.0, 0.05], 'vast': [0.0, 0.0, 0.0, 0.05], 'palace': [0.0, 0.0, 0.0, 0.05], 'of': [0.0, 0.0, 0.0, 0.05], 'versailles': [0.0, 0.0, 0.0, 0.05], 'attest': [0.0, 0.0, 0.0, 0.05], 'to': [0.0, 0.0, 0.0, 0.05]}\n"
     ]
    }
   ],
   "source": [
    "# calculando a frequencia de cada palavra nos documentos\n",
    "\n",
    "tf_matrix = {}\n",
    "\n",
    "for word in freq_words:\n",
    "    doc_tf = []\n",
    "    for data in dataset:\n",
    "        frequency = 0\n",
    "        for w in nltk.word_tokenize(data):\n",
    "            if w == word:\n",
    "                frequency += 1\n",
    "        tf_word = frequency/len(nltk.word_tokenize(data))\n",
    "        doc_tf.append(tf_word)\n",
    "    tf_matrix[word] = doc_tf\n",
    "\n",
    "print (tf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.057762265046662105, 0.03300700859809263, 0.06301338005090412, 0.03465735902799726], [0.0, 0.0806950343225908, 0.0770270782170185, 0.042364893019360174], [0.0, 0.0806950343225908, 0.0770270782170185, 0.042364893019360174], [0.0, 0.05231487088895761, 0.09987384442437362, 0.0], [0.0, 0.05231487088895761, 0.09987384442437362, 0.0], [0.0, 0.0, 0.0, 0.16094379124341004], [0.134119826036175, 0.0, 0.0, 0.0], [0.134119826036175, 0.0, 0.0, 0.0], [0.134119826036175, 0.0, 0.0, 0.0], [0.134119826036175, 0.0, 0.0, 0.0], [0.134119826036175, 0.0, 0.0, 0.0], [0.134119826036175, 0.0, 0.0, 0.0], [0.134119826036175, 0.0, 0.0, 0.0], [0.134119826036175, 0.0, 0.0, 0.0], [0.134119826036175, 0.0, 0.0, 0.0], [0.134119826036175, 0.0, 0.0, 0.0], [0.134119826036175, 0.0, 0.0, 0.0], [0.0, 0.0766399005921, 0.0, 0.0], [0.0, 0.0766399005921, 0.0, 0.0], [0.0, 0.0766399005921, 0.0, 0.0], [0.0, 0.0766399005921, 0.0, 0.0], [0.0, 0.0766399005921, 0.0, 0.0], [0.0, 0.0766399005921, 0.0, 0.0], [0.0, 0.0766399005921, 0.0, 0.0], [0.0, 0.0766399005921, 0.0, 0.0], [0.0, 0.0766399005921, 0.0, 0.0], [0.0, 0.0766399005921, 0.0, 0.0], [0.0, 0.0766399005921, 0.0, 0.0], [0.0, 0.0766399005921, 0.0, 0.0], [0.0, 0.0766399005921, 0.0, 0.0], [0.0, 0.0766399005921, 0.0, 0.0], [0.0, 0.0, 0.14631253749400913, 0.0], [0.0, 0.0, 0.14631253749400913, 0.0], [0.0, 0.0, 0.14631253749400913, 0.0], [0.0, 0.0, 0.14631253749400913, 0.0], [0.0, 0.0, 0.14631253749400913, 0.0], [0.0, 0.0, 0.14631253749400913, 0.0], [0.0, 0.0, 0.0, 0.08047189562170502], [0.0, 0.0, 0.0, 0.08047189562170502], [0.0, 0.0, 0.0, 0.08047189562170502], [0.0, 0.0, 0.0, 0.08047189562170502], [0.0, 0.0, 0.0, 0.08047189562170502], [0.0, 0.0, 0.0, 0.08047189562170502], [0.0, 0.0, 0.0, 0.08047189562170502], [0.0, 0.0, 0.0, 0.08047189562170502], [0.0, 0.0, 0.0, 0.08047189562170502], [0.0, 0.0, 0.0, 0.08047189562170502], [0.0, 0.0, 0.0, 0.08047189562170502], [0.0, 0.0, 0.0, 0.08047189562170502], [0.0, 0.0, 0.0, 0.08047189562170502]]\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF cálculo\n",
    "\n",
    "tfidf_matrix = []\n",
    "\n",
    "for word in tf_matrix.keys():\n",
    "    tfidf = []\n",
    "    for value in tf_matrix[word]:\n",
    "        score = value * word_idfs[word]\n",
    "        tfidf.append(score)\n",
    "    tfidf_matrix.append(tfidf)\n",
    "    \n",
    "print (tfidf_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
